from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# 1Ô∏è‚É£ Load FAISS index
print("üìÇ Loading FAISS index...")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

# 2Ô∏è‚É£ Load FLAN-T5 model locally (CPU mode)
print("üß† Loading local FLAN-T5 model on CPU...")
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    temperature=0,
    device=-1  # -1 = CPU
)

# 3Ô∏è‚É£ Create LLM pipeline for LangChain
llm = HuggingFacePipeline(pipeline=pipe)

# 4Ô∏è‚É£ Create RetrievalQA chain
print("ü§ñ Creating QA system...")
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# 5Ô∏è‚É£ Interactive query loop
while True:
    query = input("\n‚ùì Enter your question (or 'exit' to quit): ")
    if query.lower() == "exit":
        break

    result = qa(query)
    print("\nüì¢ Response:", result["result"])

    # Format sources neatly
    sources = list(set([doc.metadata.get("source", "Unknown") for doc in result["source_documents"]]))
    sources_str = ", ".join(sources)
    print(f"üìÑ Covered in: {sources_str if sources_str else 'No source info found'}")
